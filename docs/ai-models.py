# ğŸ¤– Guide des modÃ¨les Mistral AI

## ModÃ¨les disponibles

Le Log Analyzer supporte tous les modÃ¨les Mistral AI. Voici un guide pour choisir le bon modÃ¨le.

## ğŸ“Š Comparaison des modÃ¨les

| ModÃ¨le | Performance | CoÃ»t | Vitesse | RecommandÃ© pour |
|--------|-------------|------|---------|-----------------|
| **mistral-large-latest** | â­â­â­â­â­ | ğŸ’°ğŸ’°ğŸ’° | ğŸ¢ğŸ¢ | Production (meilleure qualitÃ©) |
| **mistral-medium-latest** | â­â­â­â­ | ğŸ’°ğŸ’° | ğŸ¢ | Production Ã©quilibrÃ©e |
| **mistral-small-latest** | â­â­â­ | ğŸ’° | ğŸš€ | Tests, faible volume |
| **open-mixtral-8x22b** | â­â­â­â­ | ğŸ’°ğŸ’° | ğŸ¢ | Open source performant |
| **open-mixtral-8x7b** | â­â­â­ | ğŸ’° | ğŸš€ | Open source Ã©conomique |
| **open-mistral-7b** | â­â­ | Gratuit | ğŸš€ğŸš€ | DÃ©veloppement/tests |

## ğŸ¯ Recommandations par cas d'usage

### Production - Haute qualitÃ©
```ini
ai_model = mistral-large-latest
ai_temperature = 0.5
ai_max_tokens = 4096
```
**Avantages :** Meilleure dÃ©tection d'anomalies, analyses plus prÃ©cises, recommandations dÃ©taillÃ©es  
**InconvÃ©nients :** CoÃ»t Ã©levÃ©, plus lent

### Production - Ã‰quilibrÃ© (RECOMMANDÃ‰)
```ini
ai_model = mistral-medium-latest
ai_temperature = 0.5
ai_max_tokens = 4096
```
**Avantages :** Bon compromis qualitÃ©/coÃ»t, performances correctes  
**InconvÃ©nients :** LÃ©gÃ¨rement moins prÃ©cis que large

### Production - Ã‰conomique
```ini
ai_model = mistral-small-latest
ai_temperature = 0.4
ai_max_tokens = 2048
```
**Avantages :** Ã‰conomique, rapide  
**InconvÃ©nients :** Moins de dÃ©tails dans les analyses

### DÃ©veloppement/Tests
```ini
ai_model = open-mistral-7b
ai_temperature = 0.5
ai_max_tokens = 2048
```
**Avantages :** Gratuit, idÃ©al pour tester  
**InconvÃ©nients :** QualitÃ© d'analyse rÃ©duite

## ğŸ’¡ Conseils d'optimisation

### RÃ©duire les coÃ»ts

1. **Utiliser un modÃ¨le plus petit pour les logs normaux**
   ```ini
   ai_model = mistral-small-latest
   ```

2. **RÃ©duire le nombre de tokens**
   ```ini
   ai_max_tokens = 2048  # Au lieu de 4096
   ```

3. **Augmenter l'intervalle de vÃ©rification**
   ```ini
   log_check_interval = 600  # 10 minutes au lieu de 5
   ```

### AmÃ©liorer la qualitÃ©

1. **Utiliser le meilleur modÃ¨le**
   ```ini
   ai_model = mistral-large-latest
   ```

2. **Augmenter les tokens pour plus de dÃ©tails**
   ```ini
   ai_max_tokens = 8192
   ```

3. **Ajuster la tempÃ©rature**
   ```ini
   ai_temperature = 0.3  # Plus dÃ©terministe
   ```

### Optimiser la vitesse

1. **ModÃ¨le rapide**
   ```ini
   ai_model = open-mixtral-8x7b
   ```

2. **Tokens limitÃ©s**
   ```ini
   ai_max_tokens = 2048
   ```

3. **TempÃ©rature basse**
   ```ini
   ai_temperature = 0.2
   ```

## ğŸ“ˆ Estimation des coÃ»ts

BasÃ© sur les tarifs Mistral AI (peut varier) :

| ModÃ¨le | Prix / 1M tokens input | Prix / 1M tokens output |
|--------|------------------------|-------------------------|
| mistral-large-latest | $2.00 | $6.00 |
| mistral-medium-latest | $0.80 | $2.40 |
| mistral-small-latest | $0.20 | $0.60 |
| open-mixtral-8x22b | $0.80 | $2.40 |
| open-mixtral-8x7b | $0.20 | $0.60 |
| open-mistral-7b | $0.10 | $0.10 |

### Exemple de calcul

Pour un serveur avec 1000 lignes de logs par heure :
- Tokens input estimÃ©s : ~500 tokens/analyse
- Tokens output estimÃ©s : ~200 tokens/analyse
- Analyses par mois : ~720 (1 toutes les heures)

**Avec mistral-large-latest :**
- Input : (0.5k Ã— 720) Ã— $2.00 / 1000 = $0.72/mois
- Output : (0.2k Ã— 720) Ã— $6.00 / 1000 = $0.86/mois
- **Total : ~$1.58/mois**

**Avec mistral-small-latest :**
- Input : (0.5k Ã— 720) Ã— $0.20 / 1000 = $0.07/mois
- Output : (0.2k Ã— 720) Ã— $0.60 / 1000 = $0.09/mois
- **Total : ~$0.16/mois**

## ğŸ”„ Changement de modÃ¨le

### Pendant le fonctionnement

1. Ã‰diter la configuration :
   ```bash
   sudo nano /etc/log_analyzer/config.ini
   ```

2. Modifier le paramÃ¨tre `ai_model`

3. RedÃ©marrer le service :
   ```bash
   sudo systemctl restart log-analyzer
   ```

4. VÃ©rifier que le nouveau modÃ¨le est utilisÃ© :
   ```bash
   sudo journalctl -u log-analyzer -n 20
   ```

### A/B Testing de modÃ¨les

Pour comparer deux modÃ¨les, vous pouvez :

1. CrÃ©er deux configurations diffÃ©rentes
2. Analyser les mÃªmes logs avec les deux
3. Comparer la qualitÃ© des analyses

## âš ï¸ Limitations par modÃ¨le

### open-mistral-7b
- Contexte limitÃ© (8k tokens max)
- Analyses moins dÃ©taillÃ©es
- Peut manquer des anomalies subtiles

### mistral-small-latest
- Bon pour les cas simples
- Moins adaptÃ© aux analyses complexes

### mistral-large-latest
- Plus coÃ»teux
- Plus lent
- NÃ©cessite plus de crÃ©dits API

## ğŸ“š Ressources

- [Documentation Mistral AI](https://docs.mistral.ai/)
- [Tarification Mistral AI](https://mistral.ai/pricing/)
- [API Reference](https://docs.mistral.ai/api/)

## ğŸ†˜ DÃ©pannage

### Erreur "Model not found"
VÃ©rifiez que le nom du modÃ¨le est correct :
```bash
python3 -c "from config_loader import load_configuration, validate_configuration; config = load_configuration(); print(validate_configuration(config))"
```

### Performances lentes
Essayez un modÃ¨le plus rapide comme `mistral-small-latest` ou `open-mixtral-8x7b`.

### CoÃ»ts Ã©levÃ©s
RÃ©duisez `ai_max_tokens` ou utilisez un modÃ¨le moins cher.